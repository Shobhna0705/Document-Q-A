{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cee69ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF for PDF processing\n",
    "import pytesseract  # Tesseract for OCR\n",
    "import textract  # Extract text from various document formats\n",
    "import spacy  # NLP library\n",
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import EntityRecognizer\n",
    "from spacy import displacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6e0041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spacy NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5fb109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract text from various document formats\n",
    "def extract_text_from_document(file_path):\n",
    "    # Extract text from PDFs using PyMuPDF\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        pdf_document = fitz.open(file_path)\n",
    "        pdf_text = \"\"\n",
    "        for page_num in range(pdf_document.page_count):\n",
    "            page = pdf_document.load_page(page_num)\n",
    "            pdf_text += page.get_text()\n",
    "        return pdf_text\n",
    "         # Extract text from common document formats using textract\n",
    "    elif file_path.endswith((\".docx\", \".doc\", \".pptx\", \".ppt\", \".xlsx\", \".xls\")):\n",
    "        text = textract.process(file_path).decode(\"utf-8\")\n",
    "        return text\n",
    "    \n",
    "     # If none of the above formats, try using Tesseract for OCR\n",
    "    else:\n",
    "        text = pytesseract.image_to_string(file_path)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85891e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to perform NLP processing on extracted text\n",
    "def nlp_processing(text):\n",
    "    # Tokenize and analyze text using Spacy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract entities (e.g., names, dates, locations)\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        entities.append({\"text\": ent.text, \"label\": ent.label_})\n",
    "\n",
    "    # Extract keywords (e.g., important terms)\n",
    "    keywords = []\n",
    "    for token in doc:\n",
    "        if not token.is_stop and not token.is_punct and not token.is_space:\n",
    "            keywords.append(token.text)\n",
    "\n",
    "    # Perform additional NLP tasks as needed (e.g., sentiment analysis, summarization)\n",
    "\n",
    "    return {\n",
    "        \"entities\": entities,\n",
    "        \"keywords\": keywords,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ace834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to process a document and perform OCR and NLP\n",
    "def process_document(file_path):\n",
    "    # Extract text from the document\n",
    "    extracted_text = extract_text_from_document(file_path)\n",
    "\n",
    "    # Perform NLP processing on the extracted text\n",
    "    nlp_result = nlp_processing(extracted_text)\n",
    "\n",
    "    return {\n",
    "        \"text\": extracted_text,\n",
    "        \"nlp_result\": nlp_result,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbfb3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    document_path = \"path_to_your_document.pdf\"\n",
    "    result = process_document(document_path)\n",
    "\n",
    "    # Specify the file path to save the results\n",
    "    output_file_path = \"output_results.txt\"\n",
    "\n",
    "    # Write the results to the specified file\n",
    "    write_results_to_file(output_file_path, result)\n",
    "\n",
    "    print(f\"Results saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821f2e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It loads text documents from a specified directory, so you should place your documents in that directory.\n",
    "#Users can input queries, and the bot will search for answers in all loaded documents.\n",
    "#The bot will provide answers based on BERT's understanding of the text within the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b5c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0d1f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbcd93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a document and return its text\n",
    "def extract_text_from_nlp_document(document_path_of):\n",
    "    with open(document_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        document_text = file.read()\n",
    "    return document_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041ca5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to answer user queries based on document content\n",
    "def answer_user_query(document_text, user_query):\n",
    "    # Tokenize the input\n",
    "    input_ids = tokenizer.encode(user_query, document_text)\n",
    "\n",
    "    # Convert tokenized input to tensors\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
    "\n",
    "    # Get BERT model's predictions\n",
    "    start_scores, end_scores = model(input_ids)\n",
    "\n",
    "    # Find the answer span with the highest probability\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "\n",
    "    # Convert token IDs back to text\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[0][answer_start:answer_end + 1]))\n",
    "\n",
    "    return answer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify the directory where your documents are located\n",
    "    documents_directory = \"path_to_documents_directory\"\n",
    "\n",
    "    # Load all documents from the specified directory\n",
    "    document_texts = {}\n",
    "    for filename in os.listdir(documents_directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            document_path = os.path.join(documents_directory, filename)\n",
    "            document_text = extract_text_from_nlp_document(output_file_path)\n",
    "            document_texts[filename] = document_text\n",
    "\n",
    "    while True:\n",
    "        user_query = input(\"Ask a question (or type 'exit' to quit): \")\n",
    "\n",
    "        if user_query.lower() == \"exit\":\n",
    "            break\n",
    "\n",
    "        # Iterate through the loaded documents and find answers\n",
    "        for doc_filename, doc_text in document_texts.items():\n",
    "            answer = answer_user_query(doc_text, user_query)\n",
    "            print(f\"Document: {doc_filename}\")\n",
    "            print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee2569e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac7c636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bef679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7009c9b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899f0d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4782c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc36808",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
